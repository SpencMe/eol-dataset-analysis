{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f86cb09f",
   "metadata": {},
   "source": [
    "# Encyclopedia of Life Dataset Catalog\n",
    "\n",
    "Systematic survey of biodiversity datasets available through the Encyclopedia of Life (EOL) open data portal.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. **Dataset Discovery**: Catalog all available datasets\n",
    "2. **Data Currency**: Assess update frequencies and recency  \n",
    "3. **Data Scale**: Determine dataset size and scope\n",
    "4. **Data Quality**: Evaluate metadata completeness\n",
    "\n",
    "## Methodology\n",
    "\n",
    "Web scraping of EOL portal with metadata extraction, statistical analysis, and quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5101741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967cf93f",
   "metadata": {},
   "source": [
    "## Dataset Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb6f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_request(url, retries=3, delay=1):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=15, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "        except requests.RequestException:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(delay * (attempt + 1))\n",
    "    return None\n",
    "\n",
    "eol_portal_url = 'https://opendata.eol.org/dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2277f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 37 pages\n"
     ]
    }
   ],
   "source": [
    "def detect_total_pages(base_url=eol_portal_url):\n",
    "    response = safe_request(base_url)\n",
    "    if not response:\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    max_page = 0\n",
    "    \n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link.get('href', '')\n",
    "        page_match = re.search(r'page=(\\d+)', href)\n",
    "        if page_match:\n",
    "            max_page = max(max_page, int(page_match.group(1)))\n",
    "    \n",
    "    return max_page if max_page > 0 else None\n",
    "\n",
    "def extract_dataset_info(element):\n",
    "    dataset = {}\n",
    "    \n",
    "    link = element.find('a') if element.name != 'a' else element\n",
    "    if link:\n",
    "        dataset['title'] = link.get_text().strip()\n",
    "        dataset['url'] = urljoin(eol_portal_url, link.get('href', ''))\n",
    "    \n",
    "    desc_elem = element.find('p') or element.find(class_='notes') or element.find(class_='description')\n",
    "    if desc_elem:\n",
    "        dataset['description'] = desc_elem.get_text().strip()[:500]\n",
    "    \n",
    "    tags = [tag.get_text().strip() for tag in element.find_all(class_=re.compile('tag|label'))]\n",
    "    dataset['tags'] = tags if tags else []\n",
    "    \n",
    "    return dataset if dataset.get('title') else None\n",
    "\n",
    "def scrape_dataset_page(page_num):\n",
    "    url = f'{eol_portal_url}?page={page_num}'\n",
    "    response = safe_request(url)\n",
    "    if not response:\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    selectors = ['li.dataset-item', 'div.dataset-item', '.package-item', 'li[class*=\"dataset\"]']\n",
    "    \n",
    "    for selector in selectors:\n",
    "        elements = soup.select(selector)\n",
    "        if elements:\n",
    "            return [extract_dataset_info(elem) for elem in elements if extract_dataset_info(elem)]\n",
    "    \n",
    "    return []\n",
    "\n",
    "total_pages = detect_total_pages()\n",
    "print(f'Detected {total_pages} pages' if total_pages else 'Using dynamic detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89c08e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 739 datasets\n"
     ]
    }
   ],
   "source": [
    "def collect_all_datasets(max_pages=None):\n",
    "    max_pages = max_pages or total_pages or 50\n",
    "    all_datasets = []\n",
    "    \n",
    "    for page in range(1, max_pages + 1):\n",
    "        datasets = scrape_dataset_page(page)\n",
    "        if not datasets and page > 5:\n",
    "            break\n",
    "        all_datasets.extend(datasets)\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return all_datasets\n",
    "\n",
    "all_datasets = collect_all_datasets()\n",
    "print(f'Collected {len(all_datasets)} datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_datasets)\n",
    "\n",
    "total_datasets = len(df)\n",
    "with_descriptions = df['description'].notna().sum()\n",
    "with_tags = df['tags'].apply(lambda x: len(x) > 0 if isinstance(x, list) else False).sum()\n",
    "with_urls = df['url'].notna().sum()\n",
    "\n",
    "print(f'Total datasets: {total_datasets}')\n",
    "print(f'With descriptions: {with_descriptions} ({with_descriptions/total_datasets*100:.1f}%)')\n",
    "print(f'With tags: {with_tags} ({with_tags/total_datasets*100:.1f}%)')\n",
    "print(f'With URLs: {with_urls} ({with_urls/total_datasets*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03744c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag frequency analysis\n",
    "all_tags = [tag for tags in df['tags'] if isinstance(tags, list) for tag in tags]\n",
    "tag_counts = pd.Series(all_tags).value_counts() if all_tags else pd.Series()\n",
    "\n",
    "# Quality assessment\n",
    "high_quality = df[\n",
    "    (df['description'].str.len() > 50) & \n",
    "    (df['tags'].apply(lambda x: len(x) > 0 if isinstance(x, list) else False)) &\n",
    "    (df['url'].notna())\n",
    "]\n",
    "\n",
    "print(f'Unique tags: {len(tag_counts)}')\n",
    "print(f'Top tags: {\", \".join(tag_counts.head(5).index.tolist()) if len(tag_counts) > 0 else \"None\"}')\n",
    "print(f'High quality datasets: {len(high_quality)} ({len(high_quality)/total_datasets*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba2687e",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9278d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "desc_lengths = df['description'].str.len().fillna(0)\n",
    "axes[0,0].hist(desc_lengths, bins=20, edgecolor='black', alpha=0.7)\n",
    "axes[0,0].set_xlabel('Description Length (characters)')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].set_title('Description Length Distribution')\n",
    "axes[0,0].grid(alpha=0.3)\n",
    "\n",
    "if len(tag_counts) > 0:\n",
    "    top_tags = tag_counts.head(10)\n",
    "    axes[0,1].barh(range(len(top_tags)), top_tags.values)\n",
    "    axes[0,1].set_yticks(range(len(top_tags)))\n",
    "    axes[0,1].set_yticklabels(top_tags.index)\n",
    "    axes[0,1].set_xlabel('Frequency')\n",
    "    axes[0,1].set_title('Top Dataset Tags')\n",
    "    axes[0,1].grid(alpha=0.3)\n",
    "else:\n",
    "    axes[0,1].text(0.5, 0.5, 'No tags found', ha='center', va='center', transform=axes[0,1].transAxes)\n",
    "\n",
    "completeness = {\n",
    "    'URLs': with_urls,\n",
    "    'Descriptions': with_descriptions, \n",
    "    'Tags': with_tags,\n",
    "    'High Quality': len(high_quality)\n",
    "}\n",
    "\n",
    "axes[1,0].bar(completeness.keys(), completeness.values())\n",
    "axes[1,0].set_ylabel('Count')\n",
    "axes[1,0].set_title('Metadata Completeness')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "axes[1,0].grid(alpha=0.3)\n",
    "\n",
    "quality_levels = ['Basic', 'Medium', 'High']\n",
    "quality_counts = [\n",
    "    df['url'].notna().sum(),\n",
    "    df[(df['description'].str.len() > 20) & (df['url'].notna())].shape[0],\n",
    "    len(high_quality)\n",
    "]\n",
    "\n",
    "axes[1,1].bar(quality_levels, quality_counts)\n",
    "axes[1,1].set_ylabel('Count')\n",
    "axes[1,1].set_title('Dataset Quality Levels')\n",
    "axes[1,1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Average description length: {desc_lengths.mean():.0f} characters')\n",
    "print(f'Quality distribution: Basic={quality_counts[0]}, Medium={quality_counts[1]}, High={quality_counts[2]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64be9ae4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Data currency and scale analysis\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m current_year = \u001b[43mdatetime\u001b[49m.now().year\n\u001b[32m      3\u001b[39m date_patterns = [\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb(20\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m, \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb(19\u001b[39m\u001b[33m\\\u001b[39m\u001b[33md\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      4\u001b[39m years_found = []\n",
      "\u001b[31mNameError\u001b[39m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "# Data currency and scale analysis\n",
    "current_year = datetime.now().year\n",
    "date_patterns = [r'\\b(20\\d{2})\\b', r'\\b(19\\d{2})\\b']\n",
    "years_found = []\n",
    "\n",
    "for desc in df['description'].fillna(''):\n",
    "    for pattern in date_patterns:\n",
    "        years_found.extend([int(year) for year in re.findall(pattern, desc)])\n",
    "\n",
    "scale_indicators = {\n",
    "    'species': int(df['description'].str.contains('species', case=False, na=False).sum()),\n",
    "    'records': int(df['description'].str.contains('records?', case=False, na=False).sum()),\n",
    "    'traits': int(df['description'].str.contains('traits?', case=False, na=False).sum()),\n",
    "    'observations': int(df['description'].str.contains('observations?', case=False, na=False).sum())\n",
    "}\n",
    "\n",
    "recent_datasets = sum(1 for year in years_found if year >= current_year - 5)\n",
    "total_with_years = len(years_found)\n",
    "\n",
    "print('Data Currency')\n",
    "print(f'Datasets with year references: {total_with_years}')\n",
    "print(f'Recent datasets (2020+): {recent_datasets}')\n",
    "print(f'Year range: {min(years_found) if years_found else \"N/A\"} - {max(years_found) if years_found else \"N/A\"}')\n",
    "\n",
    "print('\\nData Scale')\n",
    "for indicator, count in scale_indicators.items():\n",
    "    print(f'Datasets mentioning \"{indicator}\": {count}')\n",
    "\n",
    "# Export data with proper type conversion for JSON serialization\n",
    "export_data = {\n",
    "    'discovery_date': datetime.now().isoformat(),\n",
    "    'total_datasets': int(total_datasets),\n",
    "    'metadata_completeness': {\n",
    "        'with_descriptions': int(with_descriptions),\n",
    "        'with_tags': int(with_tags),\n",
    "        'with_urls': int(with_urls),\n",
    "        'high_quality': int(len(high_quality))\n",
    "    },\n",
    "    'data_currency': {\n",
    "        'datasets_with_years': total_with_years,\n",
    "        'recent_datasets': recent_datasets,\n",
    "        'year_range': [min(years_found), max(years_found)] if years_found else None\n",
    "    },\n",
    "    'data_scale': scale_indicators\n",
    "}\n",
    "\n",
    "# Correct default export path\n",
    "default_path = '../data/raw'\n",
    "os.makedirs(default_path, exist_ok=True)\n",
    "\n",
    "# Export to default location\n",
    "with open(f'{default_path}/eol_dataset_summary.json', 'w') as f:\n",
    "    json.dump(export_data, f, indent=2)\n",
    "\n",
    "df.to_csv(f'{default_path}/eol_datasets.csv', index=False)\n",
    "print(f'\\nExported {total_datasets} datasets to: {os.path.abspath(default_path)}')\n",
    "print(f'- CSV file: eol_datasets.csv ({len(df)} datasets)')\n",
    "print(f'- JSON summary: eol_dataset_summary.json (analysis results)')\n",
    "print('\\nFiles ready for analysis or sharing.')\n",
    "\n",
    "# Optional: User-specified additional save location\n",
    "print('\\n' + '='*60)\n",
    "print('OPTIONAL: Additional Export Location')\n",
    "print('='*60)\n",
    "print('You can specify an additional location to save a copy of the files.')\n",
    "print('Examples:')\n",
    "print('  - C:/Users/YourName/Documents/EOL_Data')\n",
    "print('  - ~/Desktop/biodiversity_data')  \n",
    "print('  - Leave blank to skip additional export')\n",
    "print('')\n",
    "\n",
    "# Uncomment the lines below to enable interactive export location selection:\n",
    "# additional_path = input('Enter additional save path (or press Enter to skip): ').strip()\n",
    "# \n",
    "# if additional_path:\n",
    "#     try:\n",
    "#         os.makedirs(additional_path, exist_ok=True)\n",
    "#         \n",
    "#         with open(f'{additional_path}/eol_dataset_summary.json', 'w') as f:\n",
    "#             json.dump(export_data, f, indent=2)\n",
    "#         \n",
    "#         df.to_csv(f'{additional_path}/eol_datasets.csv', index=False)\n",
    "#         print(f'✓ Successfully exported to additional location: {additional_path}')\n",
    "#         \n",
    "#     except Exception as e:\n",
    "#         print(f'✗ Error saving to {additional_path}: {e}')\n",
    "# else:\n",
    "#     print('Skipping additional export location.')\n",
    "\n",
    "print(f'\\nExport Summary:')\n",
    "print(f'- CSV file: {total_datasets} datasets with full metadata')\n",
    "print(f'- JSON summary: Complete analysis results and statistics')\n",
    "print(f'- Default location: {os.path.abspath(default_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Export to custom location using file dialog\n",
    "# Run this cell if you want to save files to an additional location\n",
    "\n",
    "try:\n",
    "    import tkinter as tk\n",
    "    from tkinter import filedialog\n",
    "    \n",
    "    print('Export to Additional Location')\n",
    "    print('='*40)\n",
    "    print('Click \"Run\" to open file dialog and choose export location.')\n",
    "    print('(A file browser window will appear)')\n",
    "    \n",
    "    # Create a root window (hidden)\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main window\n",
    "    root.attributes('-topmost', True)  # Bring dialog to front\n",
    "    \n",
    "    # Open folder selection dialog\n",
    "    custom_path = filedialog.askdirectory(\n",
    "        title=\"Select folder to export EOL dataset files\",\n",
    "        mustexist=False\n",
    "    )\n",
    "    \n",
    "    root.destroy()  # Clean up\n",
    "    \n",
    "    if custom_path:\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(custom_path, exist_ok=True)\n",
    "        \n",
    "        # Export files to chosen location\n",
    "        custom_summary_file = os.path.join(custom_path, 'eol_dataset_summary.json')\n",
    "        custom_csv_file = os.path.join(custom_path, 'eol_datasets.csv')\n",
    "        \n",
    "        with open(custom_summary_file, 'w') as f:\n",
    "            json.dump(export_data, f, indent=2)\n",
    "        \n",
    "        df.to_csv(custom_csv_file, index=False)\n",
    "        \n",
    "        print(f'✓ Successfully exported files to:')\n",
    "        print(f'  {os.path.abspath(custom_path)}')\n",
    "        print(f'  - eol_dataset_summary.json')\n",
    "        print(f'  - eol_datasets.csv')\n",
    "        \n",
    "        # Show file sizes\n",
    "        summary_size = os.path.getsize(custom_summary_file)\n",
    "        csv_size = os.path.getsize(custom_csv_file)\n",
    "        print(f'\\nFile sizes:')\n",
    "        print(f'  - Summary JSON: {summary_size:,} bytes')\n",
    "        print(f'  - Dataset CSV: {csv_size:,} bytes')\n",
    "        \n",
    "    else:\n",
    "        print('No folder selected. Export cancelled.')\n",
    "        \n",
    "except ImportError:\n",
    "    print('tkinter not available. Using text input fallback.')\n",
    "    print('\\nEnter custom export path:')\n",
    "    print('Examples:')\n",
    "    print('  Windows: C:/Users/YourName/Documents/EOL_Analysis')\n",
    "    print('  Mac/Linux: ~/Documents/EOL_Analysis')\n",
    "    \n",
    "    custom_path = input('Enter save path (or press Enter to skip): ').strip()\n",
    "    \n",
    "    if custom_path:\n",
    "        try:\n",
    "            custom_path = os.path.expanduser(custom_path)\n",
    "            os.makedirs(custom_path, exist_ok=True)\n",
    "            \n",
    "            with open(f'{custom_path}/eol_dataset_summary.json', 'w') as f:\n",
    "                json.dump(export_data, f, indent=2)\n",
    "            \n",
    "            df.to_csv(f'{custom_path}/eol_datasets.csv', index=False)\n",
    "            print(f'✓ Files exported to: {os.path.abspath(custom_path)}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'✗ Error: {e}')\n",
    "    else:\n",
    "        print('Export skipped.')\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f'✗ Error during export: {e}')\n",
    "    print('Files remain available in default location.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
